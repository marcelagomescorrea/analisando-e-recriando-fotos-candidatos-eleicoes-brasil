{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Preprocess Images"],"metadata":{"id":"MBAtbFKxfVCZ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EELyFoC8fUFQ","executionInfo":{"status":"ok","timestamp":1669237641621,"user_tz":180,"elapsed":22682,"user":{"displayName":"Hélio Bomfim de Macêdo Filho","userId":"01311604862757383692"}},"outputId":"0c10f930-3977-4394-ff5b-1c5ffe40c520"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","source":["params.py"],"metadata":{"id":"xznD8xzje9Dp"}},{"cell_type":"code","source":["AUTOENCODER_WIDTH=160\n","AUTOENCODER_HEIGHT=192\n","STATE_COLUMN_NAME = ['SG_UF']\n","YEAR_COLUMN_NAME = ['ANO_ELEICAO']\n","ID_COLUMN_NAME = ['SQ_CANDIDATO']\n","ELLECTED_COLUMN_NAME = ['DS_SIT_TOT_TURNO']\n","COLUMN_NAMES = ['NM_CANDIDATO', 'DS_CARGO'] + ID_COLUMN_NAME + ELLECTED_COLUMN_NAME + ['CD_SIT_TOT_TURNO'] + YEAR_COLUMN_NAME + STATE_COLUMN_NAME\n","FILENAME_COLUMN_NAME = ['filename']\n","FACE_COLUMN_NAME = ['face']\n","COLUMN_NAMES_FULL = COLUMN_NAMES + FILENAME_COLUMN_NAME\n","states = ['AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'ES', 'GO', 'MA', 'MT', 'MS', 'MG', 'PA', 'PB', 'PR', 'PE', 'PI', 'RJ', 'RN', 'RS', 'RO', 'RR', 'SC', 'SP', 'SE', 'TO', 'DF']\n","ELEITO = ['ELEITO', 'ELEITO POR MÉDIA']\n","NAO_ELEITO = ['NÃO ELEITO']\n","CHUNK_SIZE = 10000\n","LOCAL_DATA_PATH_SRC='/content/gdrive/MyDrive/Bootcamp_ENAP_2022/new_input'\n","LOCAL_DATA_PATH_CSV='/content/gdrive/MyDrive/Bootcamp_ENAP_2022/new_output/csv'\n","LOCAL_DATA_PATH_INPUT_IMG='/content/gdrive/MyDrive/Bootcamp_ENAP_2022/new_output/raw_img'\n","LOCAL_DATA_PATH_OUTPUT_IMG='/content/gdrive/MyDrive/Bootcamp_ENAP_2022/new_output/processed_img'\n"],"metadata":{"id":"fKyaMpVUe-GD","executionInfo":{"status":"ok","timestamp":1669237641622,"user_tz":180,"elapsed":7,"user":{"displayName":"Hélio Bomfim de Macêdo Filho","userId":"01311604862757383692"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["face_detection.py"],"metadata":{"id":"9qLfyViugEbe"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import os\n","\n","def crop_face(face):\n","  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml')\n","  gray = None\n","  if len(face.shape) == 3:\n","    gray = cv2.cvtColor(face, cv2.COLOR_RGB2GRAY)\n","  else:\n","    gray = face = np.expand_dims(face, axis=-1)\n","  faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n","  if len(faces):\n","    (x,y,w,h) = faces[0]\n","    w_slack, h_slack = w//2, h//2\n","    img_cropped = face[\n","        max(0,y-h_slack):\n","        min(face.shape[0]-1,y+h+h_slack),\n","        max(0, x-w_slack):\n","        min(face.shape[1]-1,x+w+w_slack),\n","        :]\n","    return img_cropped\n","  else:\n","    square = min(face.shape[0], face.shape[1])//2\n","    mid_height = face.shape[0]//2\n","    mid_width = face.shape[1]//2\n","    return face[mid_height-square:mid_height+square,mid_width-square:mid_width+square]\n","\n","def resize_face(face, x_max=AUTOENCODER_WIDTH, y_max=AUTOENCODER_HEIGHT):\n","  scale = min(y_max/face.shape[0], x_max/face.shape[1])\n","  return cv2.resize(face, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n","\n","def pad_face(face, x_max=AUTOENCODER_WIDTH, y_max=AUTOENCODER_HEIGHT):\n","  delta_w = x_max - face.shape[1]\n","  delta_h = y_max - face.shape[0]\n","  top, bottom = delta_h//2, delta_h-(delta_h//2)\n","  left, right = delta_w//2, delta_w-(delta_w//2)\n","  return cv2.copyMakeBorder(face, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n","\n","def gray_face(face):\n","  return cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n","\n","def is_gray(face):\n","    if len(face.shape) < 3: return True\n","    if face.shape[2]  == 1: return True\n","    b,g,r = face[:,:,0], face[:,:,1], face[:,:,2]\n","    if (b==g).all() and (b==r).all(): return True\n","    return False"],"metadata":{"id":"CZNx09-sgD0d","executionInfo":{"status":"ok","timestamp":1669237645275,"user_tz":180,"elapsed":897,"user":{"displayName":"Hélio Bomfim de Macêdo Filho","userId":"01311604862757383692"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["utils.py"],"metadata":{"id":"RSTh-fK0f6qY"}},{"cell_type":"code","source":["import os\n","\n","def get_img_filename(year: str, state: str, sq_candidato: str):\n","\n","  path = os.path.join(LOCAL_DATA_PATH_INPUT_IMG, year)\n","  options = ['F'+state+sq_candidato+'_div.jpg', 'F'+state+sq_candidato+'_div.jpeg',\n","             'F'+state+sq_candidato+'.jpg', 'F'+state+sq_candidato+'.jpeg']\n","\n","  for option in options:\n","    full_path = os.path.join(path, option)\n","    if os.path.exists(full_path): return full_path\n","\n","  return None"],"metadata":{"id":"XQ6uspBsf6Ua","executionInfo":{"status":"ok","timestamp":1669237646300,"user_tz":180,"elapsed":3,"user":{"displayName":"Hélio Bomfim de Macêdo Filho","userId":"01311604862757383692"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["local_disk.py"],"metadata":{"id":"wlwtc48Vf1i9"}},{"cell_type":"code","source":["import re\n","import os\n","import pandas as pd\n","import numpy as np\n","import zipfile\n","\n","def open_local_image(path):\n","  img = cv2.imread(path, cv2.COLOR_BGR2RGB)\n","  return img\n","\n","def save_local_image(filename: str, face, bw: bool, eleito: bool):\n","  folder = None\n","\n","  if bw:\n","    folder = os.path.join(LOCAL_DATA_PATH_OUTPUT_IMG,\n","                          'bw', 'elected' if eleito else 'not_elected')\n","  else:\n","    folder = os.path.join(LOCAL_DATA_PATH_OUTPUT_IMG,\n","                          'color', 'elected' if eleito else 'not_elected')\n","\n","  if not os.path.exists(folder):\n","    os.makedirs(folder)\n","  cv2.imwrite(os.path.join(folder, filename), face)\n","\n","  return None\n","\n","def get_pandas_chunk(year: str,\n","                     state: str,\n","                     index: int,\n","                     chunk_size: int,\n","                     verbose=True) -> pd.DataFrame:\n","    \"\"\"\n","    return a chunk of the raw dataset from local disk or cloud storage\n","    \"\"\"\n","\n","    full_path = os.path.join(\n","        LOCAL_DATA_PATH_CSV,\n","        year,\n","        f\"consulta_cand_{year}_{state}.csv\")\n","\n","    if verbose:\n","        print(f\"Source data from {full_path}: {chunk_size if chunk_size is not None else 'all'} rows (from row {index})\")\n","\n","    try:\n","        df = pd.read_csv(\n","                full_path,\n","                skiprows=np.arange(1, index+1),  # skip header\n","                nrows=chunk_size,\n","                header=0,\n","                encoding='iso-8859-1',\n","                on_bad_lines='warn',\n","                sep=';',\n","                usecols=COLUMN_NAMES)  # read all rows\n","\n","\n","        df[FILENAME_COLUMN_NAME[0]] = df[ID_COLUMN_NAME[0]].map(lambda id_candidato: get_img_filename(year, state, str(id_candidato)))\n","\n","    except pd.errors.EmptyDataError:\n","        return None  # end of data\n","\n","    return df\n","\n","def save_local_chunk(data: pd.DataFrame):\n","    \"\"\"\n","    save a chunk of the dataset to local disk\n","    \"\"\"\n","\n","    for ind in data.index:\n","        state = data[STATE_COLUMN_NAME[0]][ind]\n","        year = str(data[YEAR_COLUMN_NAME[0]][ind])\n","        eleito = data[ELLECTED_COLUMN_NAME[0]][ind]\n","\n","        if eleito not in (ELEITO + NAO_ELEITO):\n","            continue\n","\n","        sq_candidato = str(data[ID_COLUMN_NAME[0]][ind])\n","        face = data[FACE_COLUMN_NAME[0]][ind]\n","\n","        if is_gray(face):\n","            save_local_image(year+'F'+state+str(sq_candidato)+'_div.jpg', face, True, eleito in ELEITO)\n","        else:\n","            save_local_image(year+'F'+state+str(sq_candidato)+'_div.jpg', face, False, eleito in ELEITO)\n","            save_local_image(year+'F'+state+str(sq_candidato)+'_div.jpg', gray_face(face), True, eleito in ELEITO)\n","\n","\n","def extract_local_files() -> dict:\n","    def unzip_local_files(year: str, filename: str, csv: bool):\n","        from_path = os.path.join(\n","            LOCAL_DATA_PATH_SRC,\n","            year,\n","            filename)\n","\n","        zip_ref = zipfile.ZipFile(from_path, 'r')\n","\n","        to_path = os.path.join(\n","            LOCAL_DATA_PATH_CSV if csv else LOCAL_DATA_PATH_INPUT_IMG,\n","            year)\n","\n","        zip_ref.extractall(to_path)\n","        zip_ref.close()\n","\n","        if csv:\n","            states_found = []\n","            state_str = '|'.join(states)\n","            extracted_filenames = os.listdir(to_path)\n","            for extracted_filename in extracted_filenames:\n","                match = re.match(rf'.*({state_str}).csv$', extracted_filename)\n","                if match is not None:\n","                    states_found.append(match.group(1))\n","                    print(f\"{year}: ✅ found state {match.group(1)} to preprocess 👌\")\n","            return states_found\n","\n","    years = os.listdir(LOCAL_DATA_PATH_SRC)\n","    result = dict()\n","    for year in years:\n","        match = re.match(r'(\\d+)', year)\n","        if match is not None:\n","            print(f\"✅ found year {match.group(1)} to preprocess 👌\")\n","            src_year_folder = os.path.join(LOCAL_DATA_PATH_SRC, year)\n","            zipped_files = os.listdir(src_year_folder)\n","            for zipped_file in zipped_files:\n","                if zipped_file.startswith('consulta') and zipped_file.endswith('.zip'):\n","                    result[year] = unzip_local_files(year, zipped_file, csv=True)\n","                elif zipped_file.startswith('foto') and zipped_file.endswith('.zip'):\n","                    unzip_local_files(year, zipped_file, csv=False)\n","    return result\n","\n","def load_local_chunk_images(df: pd.DataFrame) -> np.ndarray:\n","  def open_local_images(filename: str) -> list:\n","    return pad_face(resize_face(crop_face(open_local_image(filename))))\n","  df[FACE_COLUMN_NAME[0]] = df[FILENAME_COLUMN_NAME[0]].map(open_local_images)\n","  return df\n","\n","\n","def get_pandas_chunk(year: str,\n","                     state: str,\n","                     index: int,\n","                     chunk_size: int,\n","                     verbose=True) -> pd.DataFrame:\n","    \"\"\"\n","    return a chunk of the raw dataset from local disk or cloud storage\n","    \"\"\"\n","\n","    full_path = os.path.join(\n","        LOCAL_DATA_PATH_CSV,\n","        year,\n","        f\"consulta_cand_{year}_{state}.csv\")\n","\n","    if verbose:\n","        print(f\"Source data from {full_path}: {chunk_size if chunk_size is not None else 'all'} rows (from row {index})\")\n","\n","    try:\n","        df = pd.read_csv(\n","                full_path,\n","                skiprows=np.arange(1, index+1),  # skip header\n","                nrows=chunk_size,\n","                header=0,\n","                encoding='iso-8859-1',\n","                on_bad_lines='warn',\n","                sep=';',\n","                usecols=COLUMN_NAMES)  # read all rows\n","\n","\n","        df[FILENAME_COLUMN_NAME[0]] = df[ID_COLUMN_NAME[0]].map(lambda id_candidato: get_img_filename(year, state, str(id_candidato)))\n","\n","    except pd.errors.EmptyDataError:\n","        return None  # end of data\n","\n","    return df\n","\n","def save_local_chunk(data: pd.DataFrame):\n","    \"\"\"\n","    save a chunk of the dataset to local disk\n","    \"\"\"\n","\n","    for ind in data.index:\n","        state = data[STATE_COLUMN_NAME[0]][ind]\n","        year = str(data[YEAR_COLUMN_NAME[0]][ind])\n","        eleito = data[ELLECTED_COLUMN_NAME[0]][ind]\n","\n","        if eleito not in (ELEITO + NAO_ELEITO):\n","            continue\n","\n","        sq_candidato = str(data[ID_COLUMN_NAME[0]][ind])\n","        face = data[FACE_COLUMN_NAME[0]][ind]\n","\n","        if is_gray(face):\n","            save_local_image(year+'F'+state+str(sq_candidato)+'_div.jpg', face, True, eleito in ELEITO)\n","        else:\n","            save_local_image(year+'F'+state+str(sq_candidato)+'_div.jpg', face, False, eleito in ELEITO)\n","            save_local_image(year+'F'+state+str(sq_candidato)+'_div.jpg', gray_face(face), True, eleito in ELEITO)\n","\n","\n","def extract_local_files() -> dict:\n","    def unzip_local_files(year: str, filename: str, csv: bool):\n","        from_path = os.path.join(\n","            LOCAL_DATA_PATH_SRC,\n","            year,\n","            filename)\n","\n","        zip_ref = zipfile.ZipFile(from_path, 'r')\n","\n","        to_path = os.path.join(\n","            LOCAL_DATA_PATH_CSV if csv else LOCAL_DATA_PATH_INPUT_IMG,\n","            year)\n","\n","        zip_ref.extractall(to_path)\n","        zip_ref.close()\n","\n","        if csv:\n","            states_found = []\n","            state_str = '|'.join(states)\n","            extracted_filenames = os.listdir(to_path)\n","            for extracted_filename in extracted_filenames:\n","                match = re.match(rf'.*({state_str}).csv$', extracted_filename)\n","                if match is not None:\n","                    states_found.append(match.group(1))\n","                    print(f\"{year}: ✅ found state {match.group(1)} to preprocess 👌\")\n","            return states_found\n","\n","    years = os.listdir(LOCAL_DATA_PATH_SRC)\n","    result = dict()\n","    for year in years:\n","        match = re.match(r'(\\d+)', year)\n","        if match is not None:\n","            print(f\"✅ found year {match.group(1)} to preprocess 👌\")\n","            src_year_folder = os.path.join(LOCAL_DATA_PATH_SRC, year)\n","            zipped_files = os.listdir(src_year_folder)\n","            for zipped_file in zipped_files:\n","                if zipped_file.startswith('consulta') and zipped_file.endswith('.zip'):\n","                    result[year] = unzip_local_files(year, zipped_file, csv=True)\n","                elif zipped_file.startswith('foto') and zipped_file.endswith('.zip'):\n","                    unzip_local_files(year, zipped_file, csv=False)\n","    return result\n","\n","def load_local_chunk_images(df: pd.DataFrame) -> np.ndarray:\n","  def open_local_images(filename: str) -> list:\n","    return pad_face(resize_face(crop_face(open_local_image(filename))))\n","  df[FACE_COLUMN_NAME[0]] = df[FILENAME_COLUMN_NAME[0]].map(open_local_images)\n","  return df\n"],"metadata":{"id":"slc63HqFfzTT","executionInfo":{"status":"ok","timestamp":1669237648516,"user_tz":180,"elapsed":465,"user":{"displayName":"Hélio Bomfim de Macêdo Filho","userId":"01311604862757383692"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["data.py"],"metadata":{"id":"uq7YtxYIjHCe"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    clean raw data by removing buggy or irrelevant transactions\n","    or columns for the training set\n","    \"\"\"\n","\n","    # remove useless/redundant columns\n","    df = df[COLUMN_NAMES_FULL]\n","    df = df.dropna(subset=FILENAME_COLUMN_NAME)\n","\n","    return df\n","\n","def get_chunk(year: str,\n","              state: str,\n","              index: int = 0,\n","              chunk_size: int = None,\n","              verbose=False) -> pd.DataFrame:\n","    \"\"\"\n","    Return a `chunk_size` rows from the source dataset, starting at row `index` (included)\n","    Always assumes `source_name` (CSV or Big Query table) have headers,\n","    and do not consider them as part of the data `index` count.\n","    \"\"\"\n","\n","    # if os.environ.get(\"DATA_SOURCE\") == \"big query\":\n","\n","    #     chunk_df = get_bq_chunk(table=source_name,\n","    #                             index=index,\n","    #                             chunk_size=chunk_size,\n","    #                             dtypes=dtypes,\n","    #                             verbose=verbose)\n","\n","    #     return chunk_df\n","\n","    chunk_df = get_pandas_chunk(year=year,\n","                                state=state,\n","                                index=index,\n","                                chunk_size=chunk_size,\n","                                verbose=verbose)\n","\n","    return chunk_df\n","\n","def save_chunk(data: pd.DataFrame) -> None:\n","    \"\"\"\n","    save chunk\n","    \"\"\"\n","\n","    # if os.environ.get(\"DATA_SOURCE\") == \"big query\":\n","\n","    #     save_bq_chunk(table=destination_name,\n","    #                   data=data,\n","    #                   is_first=is_first)\n","\n","    #     return\n","\n","    save_local_chunk(data=data)\n","\n","def extract_files() -> dict:\n","    \"\"\"\n","    extract files\n","    \"\"\"\n","\n","    # if os.environ.get(\"DATA_SOURCE\") == \"big query\":\n","    #     ...\n","    #     return\n","\n","    return extract_local_files()\n","\n","def load_chunk_images(df: pd.DataFrame) -> np.ndarray:\n","    \"\"\"\n","    load chunk images\n","    \"\"\"\n","\n","    # if os.environ.get(\"DATA_SOURCE\") == \"big query\":\n","    #     ...\n","    #     return\n","\n","    return load_local_chunk_images(df)\n"],"metadata":{"id":"OJubvrO5i93v","executionInfo":{"status":"ok","timestamp":1669237659809,"user_tz":180,"elapsed":371,"user":{"displayName":"Hélio Bomfim de Macêdo Filho","userId":"01311604862757383692"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def preprocess():\n","    year_and_states_list = extract_files()\n","    all_chunk_count = 0\n","    all_rows_count = 0\n","    all_cleaned_rows_count = 0\n","\n","    for year, states_list in year_and_states_list.items():\n","        year_chunk_count = 0\n","        year_rows_count = 0\n","        year_cleaned_rows_count = 0\n","\n","        for state in states_list:\n","\n","            # iterate on the dataset, by chunks\n","            chunk_id = 0\n","            row_count = 0\n","            cleaned_row_count = 0\n","\n","            while (True):\n","\n","                print(f\"\\n{year}, {state}: Processing chunk n°{chunk_id}...\")\n","\n","                data_chunk = get_chunk(year=year,\n","                                    state=state,\n","                                    index=chunk_id * CHUNK_SIZE,\n","                                    chunk_size=CHUNK_SIZE)\n","\n","                # Break out of while loop if data is none\n","                if data_chunk is None:\n","                    print(f\"{year}, {state}: No data in latest chunk...\")\n","                    break\n","\n","                row_count += data_chunk.shape[0]\n","\n","                data_chunk_cleaned = clean_data(data_chunk)\n","\n","                cleaned_row_count += len(data_chunk_cleaned)\n","\n","                # break out of while loop if cleaning removed all rows\n","                if len(data_chunk_cleaned) == 0:\n","                    print(f\"{year}, {state}: ❌ No cleaned data in latest chunk...\")\n","                    break\n","                else:\n","                    print(f\"{year}, {state}: ✅ data cleaned\")\n","\n","                images_processed_chunk = load_chunk_images(data_chunk_cleaned)\n","\n","                save_chunk(images_processed_chunk)\n","\n","                chunk_id += 1\n","\n","            if row_count == 0:\n","                print(f\"{year}, {state}: ✅ no new data for the preprocessing 👌\")\n","                break\n","\n","            print(f\"{year}, {state}: ✅ data processed saved entirely: {row_count} rows ({cleaned_row_count} cleaned)\")\n","            year_chunk_count += chunk_id\n","            year_rows_count += row_count\n","            year_cleaned_rows_count += cleaned_row_count\n","\n","        print(f\"{year}: ✅ data processed saved entirely: {year_chunk_count} chunks {year_rows_count} rows ({year_cleaned_rows_count} cleaned)\")\n","        all_chunk_count += year_chunk_count\n","        all_rows_count += year_rows_count\n","        all_cleaned_rows_count += year_cleaned_rows_count\n","\n","    print(f\"✅ data processed saved entirely: {all_chunk_count} chunks {all_rows_count} rows ({all_cleaned_rows_count} cleaned)\")\n","    return None\n"],"metadata":{"id":"h28Wwr2AjK-y","executionInfo":{"status":"ok","timestamp":1669237661870,"user_tz":180,"elapsed":3,"user":{"displayName":"Hélio Bomfim de Macêdo Filho","userId":"01311604862757383692"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["preprocess()"],"metadata":{"id":"MMmupWuhjPr-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1b4b48ab-26a0-4aaa-9aa2-70b84d584c2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ found year 2022 to preprocess 👌\n","2022: ✅ found state AC to preprocess 👌\n","2022: ✅ found state AL to preprocess 👌\n","2022: ✅ found state AM to preprocess 👌\n","2022: ✅ found state AP to preprocess 👌\n","2022: ✅ found state BA to preprocess 👌\n","2022: ✅ found state CE to preprocess 👌\n","2022: ✅ found state DF to preprocess 👌\n","2022: ✅ found state ES to preprocess 👌\n","2022: ✅ found state GO to preprocess 👌\n","2022: ✅ found state MA to preprocess 👌\n","2022: ✅ found state MG to preprocess 👌\n","2022: ✅ found state MS to preprocess 👌\n","2022: ✅ found state MT to preprocess 👌\n","2022: ✅ found state PA to preprocess 👌\n","2022: ✅ found state PB to preprocess 👌\n","2022: ✅ found state PE to preprocess 👌\n","2022: ✅ found state PI to preprocess 👌\n","2022: ✅ found state PR to preprocess 👌\n","2022: ✅ found state RJ to preprocess 👌\n","2022: ✅ found state RN to preprocess 👌\n","2022: ✅ found state RO to preprocess 👌\n","2022: ✅ found state RR to preprocess 👌\n","2022: ✅ found state RS to preprocess 👌\n","2022: ✅ found state SC to preprocess 👌\n","2022: ✅ found state SE to preprocess 👌\n","2022: ✅ found state SP to preprocess 👌\n","2022: ✅ found state TO to preprocess 👌\n","✅ found year 2018 to preprocess 👌\n"]}]}]}