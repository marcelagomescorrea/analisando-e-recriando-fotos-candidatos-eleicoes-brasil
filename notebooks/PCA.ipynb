{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# PCA"],"metadata":{"id":"F8xvaZAqZaLJ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPKNjsk8efiH","outputId":"ef4e75d2-bfaf-4235-fda3-35b8cc30a86f","executionInfo":{"status":"ok","timestamp":1669226391749,"user_tz":180,"elapsed":22395,"user":{"displayName":"Marcela Correa","userId":"18268533629094762181"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","source":["params.py"],"metadata":{"id":"WSVgjnBrcibn"}},{"cell_type":"code","source":["LOCAL_REGISTRY_PATH='/content/gdrive/MyDrive/Bootcamp_ENAP_2022/new_output/model'\n","LOCAL_DATA_PATH_OUTPUT_IMG='/content/gdrive/MyDrive/Bootcamp_ENAP_2022/new_output/processed_img'\n","\n","CHUNK_SIZE=10000\n","PCA_COMPONENTS=400\n","PCA_BATCH_SIZE=256\n","AUTOENCODER_WIDTH=160\n","AUTOENCODER_HEIGHT=192"],"metadata":{"id":"_xO_j6qcch2G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["registry.py"],"metadata":{"id":"AWJaZKlAb0kS"}},{"cell_type":"code","source":["import os\n","import time\n","import pickle\n","import glob\n","from sklearn.decomposition import IncrementalPCA\n","\n","def load_pca(elected: bool, bw: bool, save_copy_locally=False) -> IncrementalPCA:\n","    \"\"\"\n","    load the latest saved model, return None if no model found\n","    \"\"\"\n","\n","    print(f\"\\nLoad pca from local disk...\")\n","\n","   # get latest model version\n","    model_directory = os.path.join(LOCAL_REGISTRY_PATH,\n","        'bw' if bw else 'color',\n","        'elected' if elected else 'not_elected',\n","        'models', 'pca')\n","\n","    results = glob.glob(f\"{model_directory}/*\")\n","    if not results:\n","        print(model_directory)\n","        return None\n","\n","    model_path = sorted(results)[-1]\n","    print(f\"- path: {model_path}\")\n","\n","    with open(model_path, \"rb\") as file:\n","        print(\"\\n‚úÖ model loaded from disk\")\n","        return pickle.load(file)\n","\n","def save_pca(pca: IncrementalPCA, params: dict, elected: bool, bw: bool) -> None:\n","    \"\"\"\n","    persist trained model, params and metrics\n","    \"\"\"\n","\n","    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n","\n","    print(\"\\nSave pca to local disk...\")\n","\n","    # save params\n","    if params is not None:\n","        params_path = os.path.join(LOCAL_REGISTRY_PATH,\n","        'bw' if bw else 'color',\n","        'elected' if elected else 'not_elected',\n","        'params', 'pca')\n","        if not os.path.exists(params_path):\n","            os.makedirs(params_path)\n","        print(f\"- params path: {params_path}\")\n","        with open(os.path.join(params_path,timestamp + \".pickle\"), \"wb\") as file:\n","            pickle.dump(params, file)\n","\n","    # save model\n","    if pca is not None:\n","        model_path = os.path.join(LOCAL_REGISTRY_PATH,\n","        'bw' if bw else 'color',\n","        'elected' if elected else 'not_elected',\n","        'models', 'pca')\n","        if not os.path.exists(model_path):\n","            os.makedirs(model_path)\n","        print(f\"- model path: {model_path}\")\n","        with open(os.path.join(model_path,timestamp + \".pickle\"), \"wb\") as file:\n","            pickle.dump(pca, file)\n","\n","    print(\"\\n‚úÖ data saved locally\")\n","\n","    return None\n"],"metadata":{"id":"sUjM6RA5ZP-9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["model.py"],"metadata":{"id":"KfCCPnS2c89z"}},{"cell_type":"code","source":["from sklearn.decomposition import IncrementalPCA\n","\n","def initialize_pca(PCA_BATCH_SIZE:int, PCA_COMPONENTS:int) -> IncrementalPCA:\n","    return IncrementalPCA(batch_size=PCA_BATCH_SIZE, n_components=PCA_COMPONENTS)\n"],"metadata":{"id":"LOCmoThlZqMT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["main.py"],"metadata":{"id":"x9BY4u8pdMjM"}},{"cell_type":"code","source":["import os\n","from tensorflow.keras.utils import image_dataset_from_directory\n","from tensorflow.errors import NotFoundError\n","from tensorflow.keras.layers import Rescaling, Flatten\n","\n","def fit_pca(elected=True, bw=True):\n","    \"\"\"\n","    Train a new model on the full (already preprocessed) dataset ITERATIVELY, by loading it\n","    chunk-by-chunk, and updating the weight of the model after each chunks.\n","    Save final model once it has seen all data, and compute validation metrics on a holdout validation set\n","    common to all chunks.\n","    \"\"\"\n","    print(f\"\\n‚≠êÔ∏è use case: fit pca on {'elected' if elected else 'not elected'} candidates with {'b&w' if bw else 'color'}-images\")\n","\n","    print(\"\\nLoading preprocessed data...\")\n","\n","    folder = os.path.join(\n","        LOCAL_DATA_PATH_OUTPUT_IMG,\n","        'elected' if elected else 'not_elected',\n","        'bw' if bw else 'color')\n","\n","    normalized_images_dataset = None\n","    # load a train set\n","    try:\n","        images_dataset = image_dataset_from_directory(folder,\n","                                                      label_mode=None,\n","                                                      batch_size=CHUNK_SIZE,\n","                                                      image_size=(AUTOENCODER_HEIGHT,AUTOENCODER_WIDTH),\n","                                                      shuffle=True,\n","                                                      crop_to_aspect_ratio=True)\n","        normalization_layer = Rescaling(1./255)\n","        flatten_layer = Flatten()\n","        normalized_images_dataset = images_dataset.map(lambda x: flatten_layer(normalization_layer(x)))\n","    except NotFoundError:\n","        print(\"\\n‚úÖ no data to train\")\n","        return None\n","\n","    pca = None\n","    #pca = load_pca(elected, bw)  # production model\n","\n","    # iterate on the full dataset per chunks\n","    chunk_id = 0\n","    row_count = 0\n","\n","    for image_batch in normalized_images_dataset:\n","\n","        print(f\"\\n‚úÖ Loading and training on preprocessed chunk n¬∞{chunk_id}...\")\n","\n","        # check whether data source contain more data\n","        if image_batch.shape[0] < PCA_COMPONENTS:\n","            print(f\"\\nLast batch ({image_batch.shape[0]}) is no greater than pca components ({PCA_COMPONENTS}). It will be skipped.\")\n","            break\n","\n","        # increment trained row count\n","        chunk_row_count = image_batch.shape[0]\n","        row_count += chunk_row_count\n","\n","        # initialize pca\n","        if pca is None:\n","            pca = initialize_pca(PCA_BATCH_SIZE, PCA_COMPONENTS)\n","\n","        # train the pca incrementally\n","        pca = pca.partial_fit(image_batch)\n","\n","        chunk_id += 1\n","\n","    if row_count == 0:\n","        print(\"\\n‚úÖ no new data for the training üëå\")\n","        return\n","\n","    print(f\"\\n‚úÖ trained on {row_count} rows\")\n","\n","    # save pca\n","    save_pca(pca=pca, params=dict(PCA_BATCH_SIZE=PCA_BATCH_SIZE, PCA_COMPONENT=PCA_COMPONENTS), elected=elected, bw=bw)\n","\n","    return None\n","\n","def fit():\n","    fit_pca(True, False)\n","    fit_pca(False, False)\n","    fit_pca(True, True)\n","    fit_pca(False, True)\n"],"metadata":{"id":"3_IOxWuyZqJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#fit_pca(elected=True, bw=False)\n","fit()"],"metadata":{"id":"58-HXxymdXCP"},"execution_count":null,"outputs":[]}]}